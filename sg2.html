<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<HTML>
<HEAD>
	<META HTTP-EQUIV="CONTENT-TYPE" CONTENT="text/html; charset=utf-8">
	<TITLE>SAMSI SG2 Central</TITLE>
	<META NAME="GENERATOR" CONTENT="OpenOffice.org 3.1  (Unix)">
	<META NAME="CREATED" CONTENT="0;0">
	<META NAME="CHANGEDBY" CONTENT="Marco Ferreira">
	<META NAME="CHANGED" CONTENT="20101229;17492300">
	<META NAME="Originator" CONTENT="Microsoft Word 11">
	<META NAME="ProgId" CONTENT="Word.Document">
	<META NAME="Title" CONTENT="Bayesian Reading Group – Department of Statistics – University of Missouri – Columbia">
	<STYLE TYPE="text/css">
	<!--
		P { color: #000000 }
		A:link { color: #0000ff }
		A:visited { color: #800080 }
	-->
	</STYLE>
</HEAD>
<BODY LANG="en-US" TEXT="#000000" LINK="#0000ff" VLINK="#800080" BGCOLOR="#ffffff" DIR="LTR">
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=5><B>Subgroup 2 -
Spring 2010</B></FONT></FONT></P>
<P><BR><BR>
</P>
<P ALIGN=CENTER STYLE="page-break-before: always"><FONT FACE="Times New Roman, serif"><FONT SIZE=5><B>Meeting
discussions</B></FONT></FONT></P>
<P><BR><BR>
</P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4 STYLE="font-size: 16pt"><FONT SIZE=4><B>5/3/10
Meeting:</B></FONT> <FONT SIZE=4>Two possible research areas were
identified:</FONT></FONT></FONT></P>
<OL>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>Look at
	building hierarchical models for spatio-temporal point processes
	that involve the conditional intensity function. What would the
	implementation look like? Try sequential importance sampling or
	particle filtering.</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>Consider
	diagnostic model checking for spatio-temporal point processes.
	Schoenberg has some work on checking for nonseparability (and
	checking for nonstationarity?). If we take a Bayesian approach, then
	we might think about a point <FONT COLOR="#000000">process version
	of posterior predictive model checking.</FONT></FONT></FONT></P>
</OL>
<P><BR><BR>
</P>
<P><FONT SIZE=4><B>5/10/10 Meeting:</B></FONT> 
</P>
<OL>
	<LI><P><FONT COLOR="#000000"><FONT FACE="Times New Roman, serif"><FONT SIZE=4>Discussion
	centered around spatial and </FONT></FONT></FONT><FONT FACE="Times New Roman, serif"><FONT SIZE=4>spatio-temporal
	Bayesian hierarchical methods. </FONT></FONT>
	</P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>Matt Thaddy
	(U Chicago Business School) has a manuscript presenting particle
	filtering for spatio-temporal point processes, which we discussed.</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>We resolved
	to see if people in the SG could come up with </FONT></FONT><FONT COLOR="#000000"><FONT FACE="Times New Roman, serif"><FONT SIZE=4>a
	dataset for use in demonstrating methodology.</FONT></FONT></FONT></P>
</OL>
<P><BR><BR>
</P>
<P><FONT SIZE=4><B>5/17/10 Meeting:</B></FONT> 
</P>
<OL>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>Next week,
	Scott will discuss the paper by Kornak, Irwin, and Cressie, &quot;Spatial
	point process models of defensive strategies: Detecting changes,&quot;
	Statistical Inference for Stochastic Processes (2006) 9:31-46. The
	paper can be downloaded at:</FONT></FONT><FONT COLOR="#000000">
	</FONT><A HREF="http://www.markirwin.net/PAPERS/kic2006.pdf"><FONT COLOR="#0000ff"><FONT FACE="Times New Roman, serif"><FONT SIZE=4><U>http://www.markirwin.net/PAPERS/kic2006.pdf</U></FONT></FONT></FONT></A></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>Orietta
	suggested a possible dataset involving avalanches in the Alps, but
	it is not complete yet. She will share a draft of a project
	description she is writing, at some point in the future.</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>Renato
	suggested two possibilities:</FONT></FONT></P>
</OL>
<UL>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>Crime data
	in Belo Horizonte, Brazil.</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>Lightning
	strikes in Brazil.</FONT></FONT></P>
</UL>
<OL START=4>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>Michael
	suggested datasets involving forest fires, from areas other than the
	west coast.</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>Frank
	suggested metadata associated with ER visits in North Carolina
	(e.g., patient location, reason for ER visit) based on a
	soon-to-become-available dataset.</FONT></FONT></P>
</OL>
<P><BR><BR>
</P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4><B>5/24/10
Meeting:</B> </FONT></FONT>
</P>
<OL>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>Orietta
	shared a draft proposal she is writing for data on avalanches. She
	could see the similarity between Schoenberg's &quot;burn index&quot;
	and a possible &quot;avalanche index&quot;. After some discussion,
	it was decided that there were proprietary constraints on the data
	and we would not pursue them.</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>Michael
	contacted a Purdue U researcher in Forestry and asked about
	availability of a dataset of forest fires from East coast forests.
	The idea was to look into what Schoenberg was doing and develop his
	approach in a different setting.</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>The paper by
	Kornak et al. (2006) was discussed and interest was expressed in
	forming more powerful statistics (e.g., something based on the K
	function) to do the same type of Monte Carlo Testing to look for
	changes in spatial point pattern over time. It was suggested that a
	CUSUM-like approach could handle more than individual changes, but
	could be used in a monitoring framework. The spatial and temporal
	dependence can cause problems with distribution theory to determine
	when the process is &quot;out of control.&quot; Michael said he
	would look into it...literature in Econometrics was mentioned as a
	possible place to start. </FONT></FONT>
	</P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>The
	following week is Memorial Day in the USA and no telecon is
	scheduled.</FONT></FONT></P>
</OL>
<P><BR><BR>
</P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4><B>5/31/10
Meeting:</B> </FONT></FONT>
</P>
<OL>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>Memorial Day
	in the USA - no telecon</FONT></FONT></P>
</OL>
<P><BR><BR>
</P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4><FONT COLOR="#000000"><B>6/7/10
Meeting:</B></FONT><FONT COLOR="#000000"> </FONT></FONT></FONT>
</P>
<OL>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>There were
	four SG2 members present - others had offered apologies in advance.</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>Frank had
	sent through some correspondance about the NCDETECT dataset from
	North Carolina hospitals. SAMSI has encouraged us to pursue those as
	individuals. (It is better to wait until Frank is present before
	discussing further.)</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>Michael
	reported he had not heard from the Purdue U forester yet. We will
	wait.</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>Jun thought
	that the locations of extreme events from the NARCCAP regional
	climate models, over time, might be a reliable source for
	spatio-temporal point-process data.</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>Discussion
	centered on whether we should pursue a data-driven problem or a
	methodological problem. The general feeling among us (Renato, Jun,
	Michael, and Noel) was that data would give rise to new methods, and
	that we have a good idea of what's new based our literature survey.</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>Noel tasked
	the SAMSI locals (Jun, Michael, and Frank, with Jun as the lead), to
	look at the extremes of *precipitation* from the NARCCAP output
	(Canadian GCM and Canadian RCM) in an exploratory mode. The data
	come every 3 hours and the time step (3, or 6, or 9, or 12 or??)
	needs to be decided. They will report at the next telecon on 6/14,
	10am.</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>Michael will
	distribute some work he had done on summarizing long-memory
	dependence results for possible use in testing (e.g., in CUSUM-like
	tests discussed at the 5/24/10 Meeting). This will be discussed on
	6/14 too.</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>The next
	telecon will be on 6/14, 10am.</FONT></FONT></P>
</OL>
<P><BR><BR>
</P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4><FONT COLOR="#000000"><B>6/14/10
Meeting:</B></FONT><FONT COLOR="#000000"> </FONT></FONT></FONT>
</P>
<OL>
	<LI><P><FONT SIZE=4>Jun presented a summary of NARCCAP data that
	emphasizes extreme values. They (Jun, Frank, and Michael) took a
	fraction (1/4) of the 30 yr precip return value and thresholded the
	rainfall field, for r(s), the NARCCAP pixel at location s. Then if a
	3-hr reading is more than .25*r(s), it becomes an event of the point
	process at time t. In this way, a space-time point process is
	established.</FONT></P>
	<LI><P><FONT SIZE=4>Our interest would be to look for changes in
	numbers of events or of &quot;pattern&quot; over time. Earlier
	discussion coming from Kornak et al.'s approach could be relevant.
	*Note: Michael prepared a 2-page document on long-memory dependence
	that might be discussed next time, in relation to this.*</FONT></P>
	<LI><P><FONT SIZE=4>The actual precip. value at the event should be
	kept, as an indication of the severity of the extreme.</FONT></P>
	<LI><P><FONT SIZE=4>Scott suggested that r(s) might be considered a
	latent variable...we do have estimates of it that might be used in
	part of the data model.</FONT></P>
	<LI><P><FONT SIZE=4>It was agreed to preserve the 3-hr time period
	but perhaps to do some subsampling if gross time trends are of
	interest.</FONT></P>
	<LI><P><FONT SIZE=4>Jun's group will present some further analysis
	of the extremes data next week based on today's discussion.</FONT></P>
	<LI><P><FONT SIZE=4>Noel will be away for 6/21 and 6/28. Scott
	volunteered to chair the 6/21 telecon and Frank the 6/28 telecon.
	Each will send bullet points to Marco after the telecon.</FONT></P>
	<LI><P><FONT SIZE=4>There will be no telecon on July 5, which is a
	public holiday in the USA.</FONT></P>
</OL>
<P><BR><BR>
</P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4><FONT COLOR="#000000"><B>6/21/10
Meeting (Points 1-4 sent by Scott. Points 5-7 sent by Renato):</B></FONT><FONT COLOR="#000000">
</FONT></FONT></FONT>
</P>
<OL>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>Detailed
	discussion regarding the background of the problem took place. In
	particular, we discussed whether the value of r value, in Jun, Frank
	and Michael’s previous formulation, should be different time steps
	or the same. </FONT></FONT>
	</P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>Renato
	described his weighted K-function and how it related to the Kornak
	et al. (2006) paper.</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>We discussed
	the precipitation data and Renato posed several questions for
	clarification. We discussed the possibility of using a birth/death
	processes to add and remove points from the point process.</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>There was
	concern among the group regarding how to define what is an event.
	The problem arises from the fact that the same event could last more
	than one time period and then be “double counted”. Marco also
	suggested to try to explore the effect of double counting, as raised
	by Renato. The current plan is to plot historgrams of 3-hr or daily
	extreme values and try to check the distribution of # of extreme
	events over time (note: this is extremes that are aggregated over
	space). Iterations will occur via email and the conversation will
	resume next Monday (6/28/10).</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>In the
	precipitation dataset, we need to define what is an event. The
	dataset is a set of surfaces (represented as values in a regular
	lattice), one for each time snapshot. Each event is each single
	pixel above a threshold? Because, if so, many neighoring pixels will
	come from a single storm and then they <FONT COLOR="#000000">should
	be considered as a single event and not many as many events.</FONT></FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>Should we
	not consider a storm as a single event, located at a central pixel,
	and with an associated mark that is a polygon (circle or elipse)
	representing its extent?</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>A storm that
	is in two different positions in two successive time snapshots are a
	single event or more than one?</FONT></FONT></P>
</OL>
<P><BR><BR>
</P>
<P><FONT SIZE=4><B>6/28/10 Meeting:</B></FONT></P>
<OL>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>Discussion
	took place on how to define an extreme. We looked at histograms
	constructed by Jun. The group decided that these plots don’t
	contain the spatial information desired. Therefore he will make
	several spatial plots at different time points that allow us to
	track storm events. Jun also raised some issues of underestimation
	of the Regional Climate Model due to lack of fine resolutions.
	There's some paper indicating that 50 by 50 is too coarse, we start
	to see some reasonable pattern when the resolution is 20 by 20 km.</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>Renato
	discussed the launcher problem further and introduced the problem of
	mine fields. He said he will forward several papers from Noel. The
	concern is that we will never get real data for the launcher
	problem. It's desirable to modify the K-function or clustering idea
	and apply it to the precipitation problem.</FONT></FONT></P>
</OL>
<P><BR><BR>
</P>
<P><FONT SIZE=4><B>7/12/10 Meeting:</B></FONT></P>
<OL>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=3>The return
	time T is a 'dial' that can change the nature of the binary field,
	as follows: Fix a time t. If Z(s) is the precip. field from NARCCAP
	and W_T(s) is the quantile corresponding to T. Then we are
	considering the binary field: B_T(s) = I(Z(s)-W_T(s) &gt; 0), where
	I(.) is the indicator function. </FONT></FONT>
	</P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=3>If T is too
	small, then B_T is not indicating high extremes and will show large
	clumps of 1s. Essentially we are considering what is known as
	&quot;clipped random fields.&quot; There is some theory associated
	with these types of fields; Ben Kedem and his former student Victor
	De Oliviera worked on them. See, for example, De Oliviera, V.
	(2004). A simple model for spatial rainfall fields. Stochastic
	Environmental Research and Risk Assessment, Volume 18, Number 2 /
	April, 2004, Pages 131-140.</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=3>Noel
	suggested that the time interval stay at 3 hours and that Jun
	experiment with T to obtain a sequence of fields that looks more
	like a point pattern than a binary (clipped) random field. The
	&quot;sweet spot&quot; for T will be a somewhat subjective and Noel
	suggested that Jun, Frank, and Michael come up with something they
	could share with the rest of SG2. </FONT></FONT>
	</P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=3>Once the T
	is chosen, the spatio-temporal point process can be defined by
	B_T(s;t) = I(Z(s;t)-W_T(s) &gt; 0). The spatio-temporal point
	process involves an obvious mark: If Z(s;t)-W_T(s) &gt; 0 (i.e.,
	there is a point at s and at time t), then associate with that point
	the mark: Z(s;t). This mark could be used to predict flooding (but I
	don't think have the expertise nor the time to do that).</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=3>Noel will
	call a meeting of SG2 at the JSM. Please take a look at your travel
	plans and JSM schedules before the next </FONT></FONT><FONT COLOR="#000000"><FONT FACE="Times New Roman, serif"><FONT SIZE=3>call-in
	on 7/19.</FONT></FONT></FONT></P>
</OL>
<P><BR><BR>
</P>
<P><FONT SIZE=4><B>7/19/10 Meeting:</B></FONT></P>
<OL>
	<LI><P><FONT FACE="Times New Roman, serif">Jun will send Marco a
	formula for posting on the web (Marco will work out how to display
	the formula), explaining how he calculated the threshold values. The
	formula will be algorithmic, in that it will define all terms and
	allow anyone to make the same calculation.</FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif">Calibration of
	thresholding with delta (= time step) was discussed. Since the
	return time is based on a 24 hr period, it was decided to make delta
	= 24 hrs also. Then the only &quot;dial&quot; would be T = 30, 50,
	75, 100 etc. Jun, Frank, and Michael will look at this and get us
	0-1 maps to consider before next week's call-in.</FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif">It is expected that maps
	will involve clumps of pixels. Those clumps could be modeled as a
	Poisson cluster process (or even as a Boolean model). The
	interesting part of the analysis will be to model the dynamics, and
	Noel suggested doing this through the parameters of the spatial
	model applied at each time slice.</FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif">Meeting at JSM. Everyone
	(Scott, Jun, Frank, Michael, Noel, and maybe Marco?) was available
	4-5pm on Mon 8/2. We'll meet at 4pm, Mon. at the Registration Desk
	at the Convention Center.</FONT></P>
</OL>
<P><BR><BR>
</P>
<P><FONT SIZE=4><B>8/2/10 Meeting:</B></FONT></P>
<OL>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=3>Six of us
	met at the JSM on Mon aft. We looked at the sequence of images Jun
	had prepared for us based on a NARCCAP run...10 consecutive 3 hour
	'snapshots' during 1968 and the same during 1969.</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=3>The 1968
	images showed an E-W band of rainfall cells that consistently stayed
	over the Great Lakes region. The 1969 images showed a N-S band that
	started out in the Mississippi Valley and moved eastward in the
	usual direction of US weather systems.</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=3>It was
	suggested that the wind-velocity info. in NARCCAP might be used to
	track the location of 'parent' cells. Once the cell is set at the
	next time point, a grain-germ growth model might be applied. Notice
	that under this model, attrition as well as growth might occur... we
	saw that in the images.</FONT></FONT></P>
</OL>
<P><BR><BR>
</P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4><B>8/9/10
Meeting: </B></FONT></FONT>
</P>
<OL>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=3>During our
	telecon, we discussed the growth model; Noel forwarded an extract of
	a few pages from his book with Chris Wikle, where the model is
	defined.</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=3>The growth
	model is very flexible. A possible model, where the intensity of the
	parent cell is a function of pressure and humidity, was discussed.
	Initially, one could try a simpler model of a step-function
	intensity simply based on geographic distance from the parent cell.</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=3>Jun sent a
	third dataset from 1970, showing yet a different structure...over 6
	time periods, a single cell formed over Arkansas, grew, and worked
	its way north, leaving the NARCCAP region over Lake Ontario. Once
	again, the dynamic growth model should be flexible enough to handle
	it.</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=3>A couple of
	action items: </FONT></FONT>
	</P>
</OL>
<UL>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=3>Jun and
	Michael will work on the return-time pdf and send it to Noel.</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=3>Noel will
	write up ideas for analysis of the data based on the growth model he
	sent out. </FONT></FONT>
	</P>
</UL>
<OL START=5>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=3>Finally,
	Noel will be at the SAMSI Complex Networks workshop at the end of
	August and plans to meet with Frank and Jun and whoever else will be
	around.</FONT></FONT></P>
</OL>
<P><BR><BR>
</P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4><B>8/16/10
Meeting: </B></FONT></FONT>
</P>
<OL>
	<LI><P><FONT FACE="Times New Roman, serif">Noel discussed a
	growth-attrition model for rainfall that he distributed on 8/13/10.
	It has some similarities to a cell-growth model that he published
	some time ago: Cressie and Hulting (1992, JASA). Different features
	here include the movement of the rainfall front, the possibility of
	foci of growth from pixels nearby the rainfall front, and the
	discrete nature of the NARCCAP pixelization.</FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif">In discussion, a typo was
	discovered: The definition of L_t(s) should be L_t(s) = {
	s+c*u_t(s): -1 =&lt; c =&lt; 1 }. The original formula in the memo
	was missing the centering of the line L_t(s) at s.</FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif">Frank and Michael will
	lead discussion of the Cressie and Hulting (1992) paper next week,
	including distributing it to SG2 members. We will discuss how the
	methods might be used for the rainfall problem.</FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif">There will be no telecon
	Aug 30 (some of us unavailable) nor Sep 7 (labor Day). That time
	could be spent developing methods and analyses for the data.</FONT></P>
</OL>
<P><BR><BR>
</P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4><B>8/23/10
Meeting: </B></FONT></FONT>
</P>
<OL>
	<LI><P>Most of the telecon was spent discussing Cressie and Hulting
	(1992). Michael took the lead on the theoretical aspects, and Frank
	covered the analysis of the tumor growth data.</P>
	<LI><P>There was discussion of how this differed from Noel's model
	in his SG2 memo from the week before. Essentially, the C&amp;H
	transformation to isotropy may not be needed, since hitting
	functions could be calculated by Monte Carlo integration. And there
	is a velocity term in the model described in the SG2 memo to track
	the movement of the rainfall cells.</P>
	<LI><P>Noel will meet with Frank and Jun next week during a visit to
	SAMSI for the Complex Networks Workshop.</P>
	<LI><P>The next telecon will be 9/13, but Noel will e-mail people
	individually about their participation after talking with Frank and
	Jun.</P>
</OL>
<P><BR><BR>
</P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4><B>9/13/10
Meeting: </B></FONT></FONT>
</P>
<UL>
	<LI><P>Noel distributed a research plan in advance that we
	discussed:</P>
</UL>
<OL TYPE=I>
	<LI><P>Jun (lead) and Frank: Simulate from the model to try to make
	the simulations look like the datasets we have obtained from NARCCAP
	rainfall.</P>
	<LI><P>Michael (lead) and Scott: Do the method-of-moments
	calculations that will be used to estimate the parameters of the
	model. Decide on the &quot;test sets,&quot; K, that will be used in
	the next stages.</P>
	<LI><P>Noel (lead) and Renato: Use estimating equations from Stage
	II to design the regression methodology that will allow the
	parameters to be estimated from the data.</P>
	<LI><P>Frank (lead), Orietta, and Jun: Analyze the data to actually
	obtain numerical estimates and standard errors based on Stages II
	and III.</P>
	<LI><P>All: Interpret the results and write a paper.</P>
</OL>
<UL>
	<LI><P>Jun and Frank did some simulations and sent them before the
	meeting. Noel thought they looked encouraging and suggested the
	ellipses generated go down to just one pixel wide in the direction
	of the minor axis.</P>
	<LI><P>Michael and Scott will work on Stage II in the next two
	weeks.</P>
	<LI><P>Noel will be meeting with Renato on 10/10/10 in North
	Carolina, before the SAMSI transition workshop, and they will
	discuss (among other things) Stage III.</P>
	<LI><P>The next telecon will be Mon 9/27/10, 10am EDT.</P>
</UL>
<P><BR><BR>
</P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4><B>9/27/10
Meeting: </B></FONT></FONT>
</P>
<UL>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT COLOR="#000000"><FONT SIZE=3><SPAN STYLE="font-weight: normal">Jun
	and Frank sent a new set of simulations based on a line</SPAN></FONT></FONT><FONT COLOR="#000000">
	</FONT><FONT COLOR="#000000"><FONT SIZE=3>rather than an ellipse.
	(In fact, a line of pixels could be thought of as a very thin
	ellipse.) They looked better than the earlier simulations.</FONT></FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=3>Michael and
	Scott had sent Noel a couple of questions off-line.</FONT></FONT></P>
</UL>
<UL>
	<P><FONT FACE="Times New Roman, serif"><FONT SIZE=3>- The first was
	about transforming to isotropy. Noel didn't think this was needed as
	long as we could calculate the hitting function. Some thought would
	be needed for the test set K that would be used. The bottom line is
	to come up with a calculation that results in the hitting probabilty
	q.</FONT></FONT></P>
	<P><FONT FACE="Times New Roman, serif"><FONT SIZE=3>- The second was
	the role of p_t+1. This is directly analogous to the role of lambda
	in a Poissson model. The Bernoulli thinning process is approximately
	a Poisson process...see the paper by Besag, Milne and Zachary
	(1982), J. Appl. Probab., 19, pp. 210-216.</FONT></FONT></P>
</UL>
<UL>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=3>Noel
	suggested we think about the random set S_t+1, defined in his
	8/13/10 memo, as a product of a positive random variable R_t+1 and
	the *line* defined by the (wind velocity)X3hrs and direction. (In
	the simulations by Jun and Frank, the only source of randomness is
	the growth centers.) Michael and Scott will put iid scale factors,
	identically distributed as R_t+1 for each growth center, in their
	calculations.</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=3>The unknowns
	to estimate from the hitting probabilties are p_t+1, E(R_t+1), and
	var(R_t+1).</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=3>There are
	opportunities to have a mini-meeting at the transition workshop.
	Noel with meet with Renato on Sun 10/10/10, and perhaps Renato, Jun,
	Frank, and Noel could have a short meeting on Mon 10/11/10.</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=3>The next
	call-in will be Mon Oct 4, 10am, 919-685-9366.</FONT></FONT></P>
</UL>
<P><BR><BR>
</P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4><B>10/4/10
Meeting: </B></FONT></FONT>
</P>
<UL>
	<LI><P><SPAN STYLE="font-weight: normal">We discussed a preliminary
	document on calculating the </SPAN>hitting probabilities, that
	Michael and Scott worked on.</P>
	<LI><P>The main point of discussion was how to define the random
	sets S_t+1. Michael and Scott will try R_t+1 as iid rvs that scale
	the sets {k*u_t(s'_t+1,j): 0 lte k lte 1} [Note added by Noel: Now
	we can respect the forward direction of the wind and choose 0 lte k
	lte 1. That is, the lower limit is 0, unlike for the definition of
	L_t where the lower limit is -1.]</P>
	<LI><P>Choice of K is still to be determined. Michael will choose
	initially K to be a disk with radius x and, after a Taylor series
	approximation, the goal is to obtain a regression on covariates that
	are functions of x.</P>
	<LI><P>Noel will meet with Renato, Jun, Frank [and Orietta, late
	addition to the meeting] at lunch time on Mon 10/11.</P>
	<LI><P>The next call in will be Mon 10/18, at 10am, 919-685-9366.</P>
</UL>
<P><BR><BR>
</P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4><B>10/18/10
Meeting: </B></FONT></FONT>
</P>
<UL>
	<LI><P><SPAN STYLE="font-weight: normal">Noel reported on the
	meeting held at SAMSI on 10/11/10, </SPAN>during the Transition
	Workshop. It was realized that we need the &quot;grain&quot; vectors
	to be iid, not just independent, for the Boolean model formulas to
	work. Noel sent a pdf scan of the last 40 pages of his 1993 book (on
	about 10/14/10), and he referred to those in the discussion.</P>
	<LI><P>Michael will work on the new hitting function after he
	receives the updated memo from Noel.</P>
	<LI><P>Jun and Frank will try out another way to &quot;fatten&quot;
	the set upon which Bernoulli thinning will be appled. It is based on
	Renato's idea to drop the threshold a small amount and thin that
	set.</P>
	<LI><P>There will be no call-in on 10/25/10. The next call-in will
	be at 10am on 11/1/10.</P>
</UL>
<P><BR><BR>
</P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4><B>11/1/10
Meeting: </B></FONT></FONT>
</P>
<UL>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=3><SPAN STYLE="font-weight: normal">Noel
	had sent an updated model (with new notation and </SPAN>another
	method to &quot;fatten&quot; the set) on 10/21/10. Jun and Frank
	were going to simulate based on the new method. The updated model
	made all the grains iid random vectors of random length and random
	orientation. Michael and Scott were going to re-visit the
	calculation of the hitting function for the updated model.</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=3>Noel will
	send to Michael and Scott some ideas for the next phase of the
	hitting-function calculation.</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=3>Renato
	suggested using the wind field vectors to help in the estimation.
	Noel and he will think about this as part of their work on the
	problem.</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=3>Frank
	generated the sets based on the new method and Jun's code...he sent
	figures to Noel. Noel asked to see all stages of the generation:
	X_t, M_t (the new method, based on a threshold fraction, f), D_t,
	and finally X_t+1.</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=3>The next
	call-in will be on Mon 11/8/10 at 10am.</FONT></FONT></P>
</UL>
<P STYLE="margin-bottom: 0in"><BR>
</P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4><B>11/8/10
Meeting: </B></FONT></FONT>
</P>
<UL>
	<LI><P>Jun and Frank tried the threshold method to generate M_t. It
	looks OK. A threshold fraction of .9 was used; maybe something like
	.7 or .8 would be more suitable and lead to smaller p_t. Jun will
	read the data in again in case we want to try smaller f. Nothing
	further for Jun and Frank to do for the moment.</P>
	<LI><P>Noel had sent an updated memo to Michael and Scott, with
	further calculations for the hitting function. This was discussed
	briefly, and Noel said he would correct some parts of it before he
	sends it to everyone (see attached - 11.8.10.SG2-Memo.pdf).</P>
	<LI><P>There will be no call-in on 11/15/10. The next call-in will
	be at 10am on 11/22/10.</P>
</UL>
<P STYLE="margin-bottom: 0in"><BR>
</P>
<P STYLE="margin-bottom: 0in"><FONT FACE="Times New Roman, serif"><FONT SIZE=4><B>11/22/10
Meeting: </B></FONT></FONT>
</P>
<UL>
	<LI><P STYLE="margin-bottom: 0in"><FONT FACE="Times New Roman, serif"><FONT SIZE=4>Noel
	suggested we make some decisions about the next phase. After
	discussion, we decided to:</FONT></FONT></P>
</UL>
<UL>
	<UL>
		<P><FONT FACE="Times New Roman, serif"><FONT SIZE=3>- use the first
		version of M_t (based on L_t), rather than the thresholding. The
		problem with using an M_t based on thresholding is that the amount
		of thresholding introduces an extra parameter</FONT></FONT></P>
	</UL>
</UL>
<P STYLE="margin-left: 0.79in"><FONT FACE="Times New Roman, serif"><FONT SIZE=3>-
get the distribution of u_t based on the E-W and N-S wind data for
all pixels in M_t. Jun and Frank will undertake this.</FONT></FONT></P>
<P STYLE="margin-left: 0.79in"><FONT FACE="Times New Roman, serif"><FONT SIZE=3>-
use the hitting function to estimate the remaining parameter (p_t,
equivalently, lambda_t). Michael, Scott, and Noel will pursue
something based on Monte Carlo integration.</FONT></FONT></P>
<UL>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=3>Noel will
	next be available for a telecon on Mon Dec 20, 10am. The telecon
	line 919-685-9366 is always available at 10am on Mondays, so smaller
	telecons are possible at this time.</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=3>The next SG2
	telecon will be Mon Dec 20, 10am</FONT></FONT></P>
</UL>
<P><BR><BR>
</P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4><B>12/20/10
Meeting: </B></FONT></FONT>
</P>
<UL>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=3><FONT COLOR="#000000">Noel
	brought everyone up to date on the current state of the research.</FONT></FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=3><FONT COLOR="#000000">Jun
	and Frank produced a bivariate scatter diagram for the E-W and N-S
	wind directions for the pixels (40 pixels) in M_t(X_t) at one time
	point t. They also sent marginal histograms. The joint distribution
	seems unimodal. Michael will work with Jun and Frank to look at a
	few things:</FONT></FONT></FONT></P>
</UL>
<P STYLE="margin-left: 0.8in"><FONT FACE="Times New Roman, serif"><FONT SIZE=3>-
transform to polar coordinates? Note there is a pixel with no wind in
either direction, which would mess up the transformation.</FONT></FONT></P>
<P STYLE="margin-left: 0.8in"><FONT FACE="Times New Roman, serif"><FONT SIZE=3>-
do a kernel smooth of the joint distribution to fit a density
f_t(u_1, u_2).</FONT></FONT></P>
<P STYLE="margin-left: 0.8in"><FONT FACE="Times New Roman, serif"><FONT SIZE=3>-
use other time points nearby to come up with a better (kernel)
estimate of wind.</FONT></FONT></P>
<UL>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=3><FONT COLOR="#000000">The
	group discussed estimation of p_t by trying to implement Laslett's
	Theorem (Cressie, 1993, pp.765-770). Scott and Noel will work on
	this.</FONT></FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=3><FONT COLOR="#000000">Noel
	will draft up an outline of a manuscript based on our research and
	send it before the next meeting.</FONT></FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=3><FONT COLOR="#000000">The
	next SG2 telecon will be Mon Jan 3, 10am. The telecon line is
	919-685-9366. Until then, happy New Year!</FONT></FONT></FONT></P>
</UL>
<P><BR><BR>
</P>
<P STYLE="margin-bottom: 0in"><BR>
</P>
<P ALIGN=CENTER><FONT SIZE=5><B>Material for downloading</B></FONT></P>
<P><FONT SIZE=4><B>Formula by Jun Zhang and</B></FONT> <FONT COLOR="#000000"><FONT FACE="Times New Roman, serif"><FONT SIZE=4><B>Michael
Levine</B></FONT></FONT></FONT><FONT SIZE=4><B>: <A HREF="formulav6.pdf">Formulas
for thresholding rainfall fields</A></B></FONT></P>
<P><FONT SIZE=4><B>Model by Noel Cressie: <A HREF="SG2-memo.pdf">Suggested
random-set model for NARCCAP 3-hourly precipitation</A></B></FONT></P>
<P><FONT SIZE=4><B>Updated model by Noel Cressie:
<A HREF="10.21.10.SG2-Memo.pdf">10.21.10.SG2-Memo.pdf</A></B></FONT></P>
<P ALIGN=LEFT STYLE="margin-left: 0.03in"><FONT COLOR="#000000"><FONT FACE="Times New Roman, serif"><FONT SIZE=4><B>Hitting
function calculations by NC: </B></FONT></FONT></FONT><A HREF="11.8.10.SG2-Memo.pdf"><FONT SIZE=4><B>11.8.10.SG2-Memo.pdf</B></FONT></A></P>
<P ALIGN=CENTER STYLE="margin-left: 0.28in"><BR><BR>
</P>
<P ALIGN=CENTER STYLE="margin-left: 0.28in"><FONT FACE="Times New Roman, serif"><FONT SIZE=5><B>Annotated
bibliography</B></FONT></FONT></P>
<P ALIGN=CENTER><BR><BR>
</P>
<P ALIGN=CENTER><FONT FACE="Times New Roman, serif"><FONT SIZE=4 STYLE="font-size: 16pt"><B>2010</B></FONT></FONT></P>
<P><BR><BR>
</P>
<P ALIGN=LEFT><FONT FACE="Times New Roman, serif"><FONT SIZE=4><B>Jesper
Moller and Carlos Diaz-Avalos (2010), Structured Spatio-Temporal
Shot-Noise Cox Point Process Models with a View to Modelling Forest
Fires, Scandinavian Journal of Statistics, Vol. 37, pp 2–25</B></FONT></FONT></P>
<P ALIGN=LEFT><FONT FACE="Times New Roman, serif"><FONT SIZE=4>(By
Renato Assunção)</FONT></FONT></P>
<P ALIGN=LEFT STYLE="margin-bottom: 0in"><BR>
</P>
<P><FONT SIZE=4>The authors deal with spatial point processes data
observed every day (or other time unit). The space-time intensity
function is the product of three functions: a purely spatial
deterministic function, a purely temporal deterministic function and
a random space-time &quot;noise&quot; error function of space-time.
The main novelty is the modelling of this last term as a shot-noise
process. The authors show how their model can be simulated and derive
its main summary statistics (such as the K-function). They exemplify
how to analyse a real problem with a forest fire dataset. The
deterministic purely spatial and temporal functions are typical
log-linear functions of covariates. The shot noise is modelled as a
separable product of normal densities kernels. To fit the model, the
authors adopt a composite likelihood approach. This allows then to
work hierarchically by first maximizing one function that depends
only on the spatial deterministic function and the spatial locations,
then maximizing another function involving the fitted spatial
parameters, the temporal paremeters and the temporal events, and
finally a third function to deal with the residual shot-noise model.
For this latter part, they use a least squares approach based on the
K-function summary function. As a final minor point, they also show
how to predict future point patterns using MCMC methods that, they
claim, are more stable than those adopted for intensities based on
log-Gaussian random effect models rather than their shot-noise model.</FONT></P>
<P><BR><BR>
</P>
<P ALIGN=LEFT><BR><BR>
</P>
<P ALIGN=LEFT><BR><BR>
</P>
<P ALIGN=CENTER><FONT FACE="Times New Roman, serif"><FONT SIZE=4 STYLE="font-size: 16pt"><B>2009</B></FONT></FONT></P>
<P><BR><BR>
</P>
<P ALIGN=LEFT><FONT FACE="Times New Roman, serif"><FONT SIZE=4><B>Renato
Assuncao and Thais Correa (2009), Surveillance to detect emerging
space-time clusters, Computational Statistics and Data Analysis, Vol.
53, 2817-2830.</B></FONT></FONT></P>
<P ALIGN=LEFT STYLE="font-weight: normal"><FONT FACE="Times New Roman, serif"><FONT SIZE=4>(By
Jun Zhang) </FONT></FONT>
</P>
<P ALIGN=LEFT STYLE="font-weight: normal"><FONT FACE="Times New Roman, serif"><FONT SIZE=4>This
paper extends Shiryaev-Roberts method from temporal domain to
spatio-temporal domain. The paper is well motivated and well written.
The main idea of Shiryaev-Roberts method is based on the sum of
likelihood ratios for all possible change-point moments and
martingale theory. The major advantage of the method is that it
offers the protection against too many false alarms. I think it may
be improved in these aspects: 1) there are three tuning parameters,
providing an automatic procedure to select them would be nice. 2) the
authors could include some empirical study on the shape of the scan
zone. 3) the separable assumption is convenient for the inference
procedure. But if time is not separable from locations, then the
authors might need to argue this model could still be a good
approximation. 4) There could be delays or missing data in databases,
these issues may have to be addressed when the algorithm is applied
in real world applications.</FONT></FONT></P>
<P ALIGN=LEFT><BR><BR>
</P>
<P ALIGN=LEFT><BR><BR>
</P>
<P ALIGN=LEFT><FONT FACE="Times New Roman, serif"><FONT SIZE=4><B>Schoenberg,
F., Pompa, J., and Chang, C. (2009). A note on non-parametric and
semi-parametric modeling of wildfire hazard in Los Angeles County,
California. Environmental and Ecological Statistics, 16, 251-269.</B></FONT></FONT></P>
<P ALIGN=LEFT><FONT FACE="Times New Roman, serif"><FONT SIZE=4>(By
Frank Zou)</FONT></FONT></P>
<P ALIGN=LEFT><FONT FACE="Times New Roman, serif"><FONT SIZE=4>This
paper focuses on modeling of wildfire hazard using point processes.
The current practice is to use Burning Index (BI) as a predictor,
which is produced based on a suite of PDEs involving certain
meteorological variables. The authors proposed an alternative point
process model whose conditional intensity depends on these
meteorological variables, and try to evaluate the predictive
performance between the proposed semi-parametric model (Model 1) and
the model using BI (Model 2). The estimation procedure consists of
two parts: the spatial background \mu is estimated by kernel method
with modified likelihood cross validation for the bandwidth
selection; whereas the parametric part is estimated by using a
Nelder-Mead optimization routine. The goodness-of-fit of the two
models were compared using AIC and Model 1 tends to have lower log
likelihood in the validation data set. The authors also suggested
that the proposed model is a promising alternative to the BI model
due to its simplicity and superior predictive performance. </FONT></FONT>
</P>
<P><BR><BR>
</P>
<P><BR><BR>
</P>
<P ALIGN=CENTER><FONT FACE="Times New Roman, serif"><FONT SIZE=4 STYLE="font-size: 16pt"><B>2008</B></FONT></FONT></P>
<P><BR><BR>
</P>
<P ALIGN=LEFT><FONT SIZE=4><B>Alejandro VEEN and Frederic P.
SCHOENBERG (2008), Estimation of Space-Time Branching Process Models
in Seismology Using an EM-Type Algorithm, Journal of the American
Statistical Association, Vol 103, 614-624 </B></FONT>
</P>
<P ALIGN=LEFT STYLE="font-weight: normal"><FONT FACE="Times New Roman, serif"><FONT SIZE=4>(By
Jun Zhang) </FONT></FONT>
</P>
<P ALIGN=LEFT><FONT SIZE=4>The conditional intensity in Epidemic-Type
Aftershock Sequence (ETAS) model contains two parts: spatial
background intensity for earthquakes not triggered by other
earthquakes and spatio-temporal intensity for
earthquakes(aftershocks) directly triggered by other earthquakes. The
conventional likelihood approach estimates all of the parameters
together, and is sensitive to starting values due to the extremely
flat likelihood function around the extrema. The authors' solution is
clever: they explicitly assume that there exists an unobserved
quantity which determines whether an earthquake is triggered or not
triggered, and an EM algorithm is used to solve the parameters. One
main weakness of the paper is that the authors manually divide the
whole spatial region into subregions with homogeneous spatial
background intensity, and changing the borders of the subregions will
change the background intensity estimates. </FONT>
</P>
<P ALIGN=LEFT><BR><BR>
</P>
<P ALIGN=LEFT><BR><BR>
</P>
<P ALIGN=CENTER><FONT FACE="Times New Roman, serif"><FONT SIZE=4 STYLE="font-size: 16pt"><B>2007</B></FONT></FONT></P>
<P><BR><BR>
</P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4><B>Renato
Assuncao and Alexandra Maia (2007), A note on testing separability in
Spatial-Temporal Marked Point Processes, Biometrics 63, 290-295.</B></FONT></FONT></P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>(By Michele
Guindani)</FONT></FONT></P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>In the analysis
of Spatio-temporal marked point processes (e.g., for environmental
risk analysis), it is common to assume separability, i.e. stochastic
independence, of the marks and the spatio-temporal process. Under
separability, data analysis can be decomposed into a purely
spatio-temporal analysis and a statistical analysis of the marks'
data. On the basis of a semiparametric model, the authors derive a
score test statistic to test the separability hypothesis. Their aim
is to contrast the null separability hypothesis against local (not
extreme) departures from the null. It is shown that the score
statistic is related to some recent test statistics proposed by
Schoenberg (2004); all the statistics can be seen seen as Cramer-von
Mises -type test statistics with different weights function. More
specifically, Schoenberg's statistics weight more heavily the
discrepancies from the null in the low-intensity regions. However, no
formal discussion of the power of the test proposed is provided by
the authors; they only point out that the ``oracle'' version of their
score statistic is locally most powerful due to the general
optimality properties of score tests. In addition, the authors
highlights some problems for controlling type I error in a residual
test proposed always by Schoenber (2004). Those problems reside in
failing to take into proper account the correlation in the point
processes.</FONT></FONT></P>
<P><BR><BR>
</P>
<P><BR><BR>
</P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4><B>Vanessa
Didelez (2007), Graphical Models for Marked Point Processes <FONT COLOR="#000000">based
on Local Independence, arxiv:0710.5874v1[math.ST].</FONT></B></FONT></FONT></P>
<P><FONT SIZE=4>(By <FONT COLOR="#000000">Priscilla Greenwood</FONT>)</FONT></P>
<P><FONT SIZE=4>This paper develops several definitions around the
idea of non-causality. Local independence is defined for marked point
processes where the set of marks is the vertex set of a graph (this
last is not stated but can be inferred from the text). For example,
the local dynamic Markov property of a MPP Y with respect to a graph
G is defined by: For each vertex k, {k} is conditionally independent
of V\cl(k) given pa(k). Here pa(k) are the &quot;parents&quot; of k
in the sense of incoming directed arrows to k, cl(k) is the set
parents together with k. Hidden in the too-simple notation is that
this says every intensity (w/r the filtration of the entire MPP),
lambda_k, is measurable with respect to the filtration generated by
the process of marks in the set cl(k). The idea of delta-separation
is, roughly, that if A,B,C are vertex sets, C delta-separates A from
B in the graph G if B is locally independent from A given C &quot;in
a certain undirected graph&quot;, explained in the text. The notation
here is too simple in the same sense as the previous example. The two
things to keep in mind are that the &quot;local&quot; notion here is
about past and future time of the MPP and it is not symmetric but
relative to directed arrows of the graph. At the beginning of the
Discussion we are told that the main point of graphical models is
that &quot;they allow certain algebraic manipulations to be replaced
by graphical ones&quot;. I had expected (hoped) that the graphical
structure would play the role of space in a space-time model. However
this paper is only about how one can define the concept of
non-causality in a number of ways which turn out to be equivalent. </FONT>
</P>
<P><BR><BR>
</P>
<P><BR><BR>
</P>
<P><FONT SIZE=4><B>Kottas and Sanso (2007), Bayesian mixture modeling
for spatial Poisson process intensities, with applications to extreme
value analysis, Journal of Statistical Planning and Inference, 137,
3151 – 3163.</B></FONT></P>
<P ALIGN=LEFT><FONT FACE="Times New Roman, serif"><FONT SIZE=4>(By
Renato Assunção)</FONT></FONT></P>
<P><FONT SIZE=4>This is not a space-time but a purely spatial paper.
The authors work with Cox processes: the events follow a
inhomogeneous Poisson point process with intensity S(x); the
intensity is random and represented as a mixture of bivariate beta
distributions. The novelty in the paper is the application of
Dirichlet process prior to model the random surface. No outstanding
problems appear because the authors reduce the intensity estimation
to a bivariate density estimation problem. One advantage of this
method is the natural way to deal with edge effects: the estimation
of the intensity near the boundaries is nicely tackled by the
polygon-based support of the beta distributions. The authors do not
mention extensions of their methods to the space-time setting.</FONT></P>
<P><BR><BR>
</P>
<P><BR><BR>
</P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4><B>Schoenberg,
F.P., C-H. Change, J.E. Keeley, J. Pompa, J. Woods, and H. Xu (2007).
A critical assessment of the burning index in Los Angeles County,
California. International Journal of Wildland Fire 16:473–483.</B></FONT></FONT>
</P>
<P STYLE="font-weight: normal"><FONT FACE="Times New Roman, serif"><FONT SIZE=4>(By
Orietta Nicolis)</FONT></FONT></P>
<P STYLE="font-weight: normal"><FONT FACE="Times New Roman, serif"><FONT SIZE=4>The
paper evaluates the empirical relationship between the Burning Index
and wildfire activity in Los Angeles County, California. The Burning
Index (BI) is a commonly used as predictor of wildfire activity and
it is based on different variables (fuel structure, weather and
antecedent drought) and on a suite of physics-based nonlinear dynamic
equations. The weather variables are recorded by Remote Automatic
Weather Stations (RAWS) implemented by the Forest Service across the
United States. In order to assess whether the BI uses weather
information optimally in the sense of predicting wildfire activity,
the predictive performance of different marked point process models
for wildfire incidence with BI as covariate, are investigated. These
results are then compared with marked point process models where the
covariates are given by weather variables (maximum relative humidity,
wind speed, precipitation, and temperature). Results reveals that
although the BI is positively associated with wildfire occurrence,
its predictive value is quite limited. In particular: (i) wind speed
alone has a higher correlation with burn area than BI and (ii) the BI
is generally far too high in Winter and too low in Fall.<BR></FONT></FONT><BR><BR>
</P>
<P><BR><BR>
</P>
<P><BR><BR>
</P>
<P ALIGN=CENTER><FONT FACE="Times New Roman, serif"><FONT SIZE=4 STYLE="font-size: 16pt"><B>2006</B></FONT></FONT></P>
<P><BR><BR>
</P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4><B>Peter J.
Diggle (2006), Spatio-temporal point processes, partial likelihood,
foot and mouth disease, Statistical Methods in Medical Research 15,
325-336.</B></FONT></FONT></P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>(By Michele
Guindani)</FONT></FONT></P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>The author
discusses the use of a partial-likelihood approach for the analysis
of spatio-temporal point process data. The partial-likelihood
approach is computationally straightforward and represents a direct
adaptation to the space-time setting of the Cox proportional hazard
models in survival data analysis, therefore it inherits their general
features. More precisely, estimates obtained by maximizing the
partial likelihood inherit the general asymptotic properties of
maximum likelihood estimators, although they may be less efficient.
Also, some of the parameters of the original models may be
unidentifiable from the partial likelihood; this may not represent a
problem if the non-identified parameters are nuisance parameters. For
example, if the time-ordering of the events is uninformative, as it
happens when the intensity of a Poisson Process has independent
spatio and temporal components, the partial likelihood method fails.
From a methodological perspective, the approach is more widely
applicable to spatio-temporal processes for which the scientific
focus is on the investigation of space-time interaction.</FONT></FONT></P>
<UL>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4><I>(Comment
	by Renato Assunção) The paper claims in passing that the usual
	properties of the Cox partial likelihood in survival analysis are
	transfered to the space-time point process situation. However, this
	is not clear. These properties are asymptotic in nature: many
	individuals with the single (or multiple) event being independently
	observed (or censored) in each one of them. That is, the properties
	are derived based on the observation of many independent point
	processes. In the present paper, we have only one single realization
	of a space-time <FONT COLOR="#000000">point process.</FONT></I></FONT></FONT></P>
</UL>
<P><BR><BR>
</P>
<P><BR><BR>
</P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4><B>K. Helisova
(2006), Spatial and Spatio-temporal Point Processes, WDS'06
Proceedings of Contributed Papers, Part I, 88-93 </B></FONT></FONT>
</P>
<P STYLE="font-weight: normal"><FONT FACE="Times New Roman, serif"><FONT SIZE=4>(By
Jun Zhang) </FONT></FONT>
</P>
<P><FONT FACE="Calibri, Verdana, Helvetica, Arial"><FONT SIZE=2 STYLE="font-size: 11pt"><FONT COLOR="#000000"><FONT FACE="Times New Roman, serif"><FONT SIZE=4><SPAN STYLE="font-weight: normal">This
paper lists some basic concepts and theoretic results in spatial and
spatio-temporal processes. It could be used as a brief introductory
review for researchers who are not familiar with point processes. The
key concept is doubly stochastic Poisson process or Cox process. The
author also describes a simulation algorithm for the spatio-temporal
Cox process driven by Ornstein-Uhlenbeck process. </SPAN></FONT></FONT></FONT><B><BR></B></FONT></FONT><BR><BR>
</P>
<P><BR><BR>
</P>
<P><FONT SIZE=4><B>Kornak, J., Irwin, M. and Cressie, N. (2006),
Spatial Point Process Models of Defensive Strategies: Detecting
Changes, <I>Statistical Inference for Stochastic Processes</I>, 9,
31-46.</B></FONT></P>
<P><FONT SIZE=4>(By Scott Holan)</FONT></P>
<P><FONT SIZE=4>This paper develops several Monte Carlo tests for
detecting change in stochastic processes. Specifically, the authors
consider a variety of different test statistics, derived in the
spatial point process context, to evaluate change in a spatial
process from one time point to another. Further, the authors consider
both global and local tests. The test statistics are developed from
the spatial variance and average intensity as computed from
estimating the intensity surface using a kernel approach. The
authors’ objective approach to choosing the bandwidth in this
context was particularly appealing. In general the paper is well
written and provides a nice framework for testing.</FONT></P>
<P><BR><BR>
</P>
<P><BR><BR>
</P>
<P><FONT SIZE=4><B>Särkkä, A., Renshaw, E. (2006) The analysis of
marked point patterns evolving through space and time. Computational
Statistics &amp; Data Analysis 51, 1698 – 1718.</B></FONT></P>
<P STYLE="font-weight: normal"><FONT FACE="Times New Roman, serif"><FONT SIZE=4>(By
Orietta Nicolis)</FONT></FONT></P>
<P><FONT SIZE=4>This paper develops a highly flexible and
computationally fast generator of space–time structure by combining
a deterministic growth/interaction term with a stochastic
immigration–death process. Differently to the work of Renshaw, E.,
Sarkka, A., 2001, the parameters of the marked point process are
estimated through time and the least squares estimation is employed.
Simple stochastic models are used to show that least squares methods
are as powerful as likelihood-based approaches, as well as being
mathematically and computationally simpler. Then, a simulation study
is performed for the estimation of parameters of two
growth-interaction models exhibiting the greatest ‘behavioural
distance’: the symmetric hard-core interaction process combined
with logistic growth function (model A) and the non-symmetric
area-interaction process combined with a linear growth function
(model D’) . Finally, the procedures A and D’ are applied to the
analysis of a new Swedish pine forest data set for which tree
location and diameter at breast height were recorded in 1985, 1990
and 1996. The results show that although model D’ appears to be
more appropriate than model A for these data, a simple residual
analysis suggests further improvement to the model.</FONT></P>
<P><BR><BR>
</P>
<P><BR><BR>
</P>
<P><BR><BR>
</P>
<P ALIGN=CENTER><FONT FACE="Times New Roman, serif"><FONT SIZE=4 STYLE="font-size: 16pt"><B>2005</B></FONT></FONT></P>
<P ALIGN=CENTER><BR><BR>
</P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4><B>Diggle, P.,
Rowlingson, B., and Su, T. (2005), Point process methodology for
on-line spatio-temporal disease surveillance, Environmetrics 16,
423-434.</B></FONT></FONT></P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>(By Frank Zou)</FONT></FONT></P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>This paper
proposed a Bayesian hierarchical point process model to detect
spatial and temporal anomaly in disease counts. The basic idea is to
model the intensity function with three multiplicative components:
overall spatial variation \lambda_0(x), temporal variation \mu_0(t),
and the residual space-time interaction term. The first two
deterministic \lambda_0(x) and \mu_0(t) are estimated by using an
adaptive bandwidth Gaussian kernel and Poisson log-linear regression
model respectively. The space-time dependence R(x, t) is modeled by a
unit-mean log-Gaussian point process with the exponent S(x, t) being
a stationary Gaussian process. Estimation of the correlation function
is done by empirical moment matching. The spatio-temporal prediction
is achieved by approximating the predictive exceedance probability
via sampling from the posterior predictive distribution. </FONT></FONT>
</P>
<P><BR><BR>
</P>
<P><BR><BR>
</P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4><B>Frederic P.
Schoenberg (2005), Consistent parametric estimation of the intensity
of a spatial-temporal point process, JSPI 128, pp.79-93. </B></FONT></FONT>
</P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>(By <FONT COLOR="#000000">Michael
Levine</FONT>)</FONT></FONT></P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>This is a nice
paper that is dedicated to parametric MLE and pseudo-MLE approach to
estimating intensity of various spatial-temporal point processes. The
case considered is purely parametric - the intensity function is
fully described by a finite-dimensional parameter. The case is made
that, quite often, estimating just intensity function is all that
matters, in particular where the mean behavior of a point process is
of interest. The author is claiming that, as opposed to the case of
conditional intensity, it is relatively easy to derive consistency
result for pseudo-MLE of the process intensity parameter under not
very restrictive conditions. In particular, these conditions do not
involve second and third order partial derivatives of pseudo-MLE that
are hard to verify in practice. Under very similar set of conditions,
the author also obtains asymptotic consistency of the weighted least
squares estimator of the same parameter. The author also provides a
nice selection of specific processes that satisfy these assumptions
as well those that do not. In particular, the author shows that
spatial-temporal versions of both separable cyclic and non-cyclic
non-separable Poisson processes satisfy the needed assumption under
minimal restrictions on the process intensity. </FONT></FONT>
</P>
<P><BR><BR>
</P>
<P><BR><BR>
</P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4><B>Peng, R.,
Schoenberg, F. and Woods, J. (2005). A space-time conditional
intensity model for evaluating a wildfire hazard index. <FONT COLOR="#000000">Journal
of the American Statistical Association, 100, 26 - 35.</FONT></B></FONT></FONT></P>
<P ALIGN=LEFT><FONT FACE="Times New Roman, serif"><FONT SIZE=4>(By
Renato Assunção)</FONT></FONT></P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>In this paper no
new methodological developments are presented. It does a data
analysis of wildfire events in Los Angeles county to answer some
practical questions. There is a Burning Index (BI) that, using fuel
and meteorological data gathered from weather stations, was
originally calculated to predict the flame length, a measure of fire
intensity <FONT COLOR="#000000">(see
</FONT><FONT COLOR="#0000ff"><U>http://www.forestencyclopedia.net/p/p467</U></FONT><FONT COLOR="#000000"><SPAN STYLE="text-decoration: none">
for an illustrative figure). </SPAN></FONT>This BI index has been
used to predict fire occurrence rather than fire intensity. The
interest of the authors is to verify either BI is able to predict
fire occurrence. Ths answer is that it has a small marginal
predictive power, as explained below. The authors fit two main
models. One has a conditional intensity that is simply the addition
of a purely spatial function and a purely temporal (seasonal within
the year) function. The second one incorporates the BI index
additionally by using a BI surface interpolated out of daily
measurements of BI from 8 weather stations spread on the region. The
fit is traditional (MLE) as well as the model comparison (AIC). They
also use residual analysis (random thinning methods and rescaling
methods) which are more recent and that were proposed in other
papers. The conclusion is that the BI model is only marginally better
than the non-BI model. The residual analysis show that both models
are not able to capture some specific oscialltions of intensity
during the years 1978-1982 (too many fires occurred) and the BI model
tend to overpredict fires during the winter-spring period.</FONT></FONT></P>
<P><BR><BR>
</P>
<UL>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4><I>(Comment
	by Renato Assunção)</I></FONT></FONT> <FONT FACE="Times New Roman, serif"><FONT SIZE=4>They
	use a highly artificial Ripley-K space-time function by &quot;equating&quot;
	5 miles to 4 years. The argument is unconvincing and they could have
	used a space-time K(d,t) surface as Diggle et al 1995).</FONT></FONT></P>
	<LI><P><FONT FACE="Times New Roman, serif"><FONT SIZE=4><I>(Comment
	by Renato Assunção)</I></FONT></FONT> <FONT FACE="Times New Roman, serif"><FONT SIZE=4>They
	also fitted multiplicative models (conditional intensity is the
	product of purely spatial and purely temporal and BI surface at time
	t). This would be more in line with typical applications. They claim
	they got reasonable fit but only AIC numbers are shown, which are
	not comparable to those from the other </FONT></FONT><FONT COLOR="#000000"><SPAN STYLE="text-decoration: none"><FONT FACE="Times New Roman, serif"><FONT SIZE=4>additive
	models because they are non-nested models.</FONT></FONT></SPAN></FONT></P>
</UL>
<P><BR><BR>
</P>
<P><BR><BR>
</P>
<P ALIGN=CENTER><FONT FACE="Times New Roman, serif"><FONT SIZE=4 STYLE="font-size: 16pt"><B>2004</B></FONT></FONT></P>
<P><BR><BR>
</P>
<P STYLE="margin-bottom: 0in"><BR>
</P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4><B>Ogata (2004)
Space-time model for regional seismicity and detection of crustal
stress changes. Journal of Geophysical Research, 109, No. B3, B03308,
<FONT COLOR="#000000">doi:10.1029/2003JB002621.</FONT></B></FONT></FONT></P>
<P ALIGN=LEFT><FONT FACE="Times New Roman, serif"><FONT SIZE=4>(By
Renato Assunção)</FONT></FONT></P>
<P><FONT SIZE=4>This is one of a long series of papers using the ETAS
(Epidemic Type Aftershock-Sequences) model proposed by the author. In
this model, only earthquakes above a certain magnitude M_0 is
considered. There is an underlying constant rate of ocurrence of
earthquakes. On top of this, each random earthquake immediately adds
some quantity to this intensity rate. This additional quantity varies
in time by decreasing to zero as times passes. This decrease is
polynomial in the time elapsed and it depends positively on the on
the earthquake magnitude. He allows this model to have spatially
varying parameters using an ingenious Delaunay tesselation approach.
The main objective of the paper is to detect regions where this ETAS
model does not fit the data well. Regions and moments of lower than
expected intensity would anomalous and indicate quiet periods. There
is scientific interest in the disputed theory that relatively quiet
periods immediately precede large earthquakes. He finds a temporally
changing spatial surface that indicates the degree of quiescence. I
was concerned about the identifiability of these surfaces but I may
not have understood it entirely. The paper is inconclusive with
respect to the quiescence theory as evidence in this paper and in
other previous papers show disparate evidence.</FONT></P>
<P ALIGN=LEFT><BR><BR>
</P>
<P ALIGN=LEFT><BR><BR>
</P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4><B>Schoenberg, F.
P. (2004) Testing Separability in Spatio-Temporal Marked Point
Processes, <I>Biometrics</I>, 60, 471-481.</B></FONT></FONT></P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>(By <FONT COLOR="#000000">Scott
Holan</FONT>)</FONT></FONT></P>
<P><FONT SIZE=4>This paper is concerned with testing separability in
spatio-temporal marked point process. Specifically, several
nonparametric tests are considered for separability of the mark and
conditional spatial-temporal intensities. To formulate the
nonparametric estimates the author uses multidimensional kernel
densities (fixing the kernel specification, the bandwidth and
ignoring edge effects). The author highlights two tests. The first
test, a Cramer-Von Mises test, is direct test whereas the second test
considers the L-function applied to the rescaled residuals. The tests
are conducted by finding Monte Carlo critical values through
simulation under the null hypothesis. The author provides in
indication of the power of the different tests considered under
different alternative specifications. Finally, the author
demonstrates the methodology through a real data example, modeling
Los Angeles County wildfires.</FONT></P>
<P><BR><BR>
</P>
<P><BR><BR>
</P>
<P ALIGN=CENTER><FONT FACE="Times New Roman, serif"><FONT SIZE=4 STYLE="font-size: 16pt"><B>2002</B></FONT></FONT></P>
<P><BR><BR>
</P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4><B>A. Brix, J.
Chadeuf (2002), Spatio-Temporal Modelling of Weeds by Shot-Noise G
Cox Processes, Biometrical Journal 44, 1, pp.83-99. </B></FONT></FONT>
</P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>(By <FONT COLOR="#000000">Michael
Levine</FONT>) </FONT></FONT>
</P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>This paper
purports to describe how to use shot-noise G Cox processes to model
weed distribution in the field. The introduction is dedicated to the
description of the problem of weed control and why the
geostatistics-type methods are not suitable for its analysis since
they only model large scale properties. Some speficic problems, such
as recognition of weeds in images are discussed. This is followed by
the description of the data. The paper suffers from low quality
English which makes it sometimes hard to read. The middle part of the
paper is devoted to the description of the shot-noise G Cox
processes. This part represents a shortened and simplified version of
the paper &quot;Generalized Gamma Measures and Shot Noise Cox
Processes&quot; by A. Brix in the Advances in Applied Probability in
1999. Extreme conciseness of this section makes it hard to read.
Finally, the last part of the paper introduces some simple moment
estimators of the parameters of the process considered and uses the
real data example of the weed distribution to obtain estimates. No
theoretical analysis of the proposed estimators is being performed. </FONT></FONT>
</P>
<P><BR><BR>
</P>
<P><BR><BR>
</P>
<P><BR><BR>
</P>
<P ALIGN=CENTER><FONT FACE="Times New Roman, serif"><FONT SIZE=4 STYLE="font-size: 16pt"><B>2001</B></FONT></FONT></P>
<P><BR><BR>
</P>
<P><FONT SIZE=4><B>Brix, A. and Diggle, P. (2001), Spatiotemporal
prediction for log-Gaussian Cox Processes, <I>Journal of the Royal
Statistical Society – Series B,</I> 63, 823—841.</B></FONT></P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>(By <FONT COLOR="#000000">Scott
Holan</FONT>)</FONT></FONT></P>
<P><FONT SIZE=4>This paper develops a model for space-time point
pattern data. The models developed by the authors are Cox processes
with an Ornstein-Uhlenbeck space-time stochastic intensity function.
Specifically, the authors focus on formulation of a point process
model for describing and predicting space-time variation in the
intensity and developing methodology for parameter estimation. The
type of data the authors are concerned with are recorded over
discrete time intervals, almost continuously (resulting in large
amounts of data) and predictions are required in real-time. Due to
computational difficulties the authors assume separability and
develop an estimation approach based on the method of moments and use
a Metropolis adjusted Langevin algorithm. Several additional
simplifying model assumptions are made. Further, the authors modeling
approach requires several user defined parameters (choices). Finally,
the authors do not demonstrate their approach on actual data.
Instead, they simulate data claimed to behave like data that
motivated the research.</FONT></P>
<P><BR><BR>
</P>
<P><BR><BR>
</P>
<P ALIGN=CENTER><FONT FACE="Times New Roman, serif"><FONT SIZE=4 STYLE="font-size: 16pt"><B>1999</B></FONT></FONT></P>
<P><BR><BR>
</P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4><B>A. Brix
(1999), Generalized Gamma Measures and Shot Noise Cox Processes,
Advances in Applied Probability, vol. 31, 929-953 .</B></FONT></FONT></P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>(By <FONT COLOR="#000000">Michael
Levine</FONT>) </FONT></FONT>
</P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4>The paper
&quot;Generalized Gamma Measures and Shot-Noise Cox Processes&quot;
by Anders Brix suggests a family of possibly stochastic measures that
can be used to model spatial intensity of Cox processes. Note that
the regular point processes are used to model clustered point
patterns; while doing so, they assume implicitly that the counts of
points are not overdispersed. This is not true quite regularly and,
therefore, there is a need for a spatial model that can handle
&quot;spatial overdispersion&quot;. This can be done by making the
intensity of the underlying Poisson process to be stochastic - this
is where Cox processes come into picture. The author considers a
special type of the Cox process - the process with the so-called
shot-noise G-measure as an intensity measure. The author starts with
an introduction of the G-family of distributions. This family is
characterized by its Laplace transform depending on three parameters:
index parameter, shaper parameter and intensity parameter. Then, a
family of measures is introduced by allowing the intensity parameter
to be a function of a set and assuming that the resulting measure has
independent increments. It can be shown using earlier results that
thusly defined measure exists, is consistent and continuous. Results
considered here are heavily based on the fact that G-family of
distributions is infinitely divisible in the intensity parameter -
the consistency of the resulting measure follows from this. It is
shown that in the practically important case where the index
parameter is less than 1, the resulting measure has a Poisson-type
representation, is purely atomic a.s., stationary iff the underlying
spatial intensity measure is proportional to the Lebesgue measure
and, finally, has no fixed atoms iff the underlying spatial intensity
measure is diffuse. As a next step, the author notes that purely
atomic measures may be inadequate to model, say, positions of trees
and weeds. Therefore, he proposes to smooth a G-measure using some
smoothing kernel. Several important cases are considered, including
the so-called standard shot-noise measures that can be thought of as
the result of linear smoothing of a G-measure. Two important results
are proved for general shot-noise measures: 1) Their cumulant measure
is calculated explicitly, thus providing all of the moments; 2) In a
special case of the convolution kernel used to construct a shot-noise
measure, the resulting measure is shown to be stationary and mixing.
The author considers some possible ways of simulating shot-noise
G-measures. If the smoothing kernel is linear, a simple direct
simulation on the grid is proposed. Otherwise, an indirect simulation
of the atoms of a G-measure is suggested. In the final section of the
paper, the author considers two types of resulting Cox processes. The
first are processes whose intensity measure is the &quot;original&quot;
unsmoothed G-measure. Such a process can be shown to be a marked
Poisson process. The second group consists of processes whose
intensity is modeled using smoothed shot-noise G-measures. In several
special cases, the author derives an approximate likelihood. Finally,
the author suggests that in many practical applications (such as
modeling the distribution of weeds) the nearest neighbor Markov
property may be a natural feature and that a shot-noise Cox process
may, under some technical conditions, possess the nearest neighbor
Markov property.</FONT></FONT></P>
<P><BR><BR>
</P>
<P ALIGN=CENTER><FONT FACE="Times New Roman, serif"><FONT SIZE=4 STYLE="font-size: 16pt"><B>1998</B></FONT></FONT></P>
<P><FONT FACE="Times New Roman, serif"><FONT SIZE=4><B><FONT COLOR="#000000">Yosihiko
Ogata (1998), Space-time point process models for earthquake
occurences, </FONT>Ann Inst Statist Math <FONT COLOR="#000000">50,
379-402</FONT>.</B></FONT></FONT></P>
<P><FONT SIZE=4>(By <FONT COLOR="#000000">Priscilla Greenwood</FONT>)</FONT></P>
<P><FONT SIZE=4>Three space-time versions of the Epidemic-Type
Aftershock-Sequences (ETAS) model of Ogata (1988) are defined, using
a Hawkes-type pattern, and compared using the AIC (Akaike Information
Criterion) with earthquake data from two regions, A and B, of Japan.
The conditional intensity function has base intensity mu(x.y) and
parametric response functions which include quadratic ( perhaps
Gaussian-type) spatial factors and exponential decay in time and
quake magnitude. The parameters of these models are mu and the
collection, phi, of the various exponential parameters and variances
in the response function. There is an extra power of t+c in the
denominator of each response function. The objective of the study is
to indicate anomalous areas and any corresponding temporal anomalies.
An aftershock region is known to correspond to a ruptured fault
region of the main shock. This is incorporated into the model in such
a way that one can infer the extent of these regions, by choice of
response function and parameter estimation. A constraint on the model
is that the space integral of the response function should yield back
the ETAS. A technical point in the log likelihood computation is the
division of the neighborhood of each magnitude-selected shock into
pi-like sectors and integration using polar-like coordinates. Another
variation is the introduction of anisotropy. The AIC selects the
third proposed model, the one with most speard-out ruptured regions,
and a conclusion is that decay is long-range with distance. By
breaking down into models that look at earthquakes over magnitude M
for several values of M, Ogata finds that &quot;clustering activity
of clusters&quot; is higher in his region A than in region B.
Simulation shows that real seismaticity yields synchronous and high
intensity clusters more than data simulated from fitted models. The
main results are the long range and power law decay of aftershock
regions. There may be two components present, with near and far field
aspects. Extension of the models to non-homogeneous background have
not changed the results. Ogata mentions estimating covariance
structure for these models. An interesting exercise toward
understanding the models would be to compute their covariance
structures for some choices of parameter. and compare some plots of
these.</FONT></P>
<P><BR><BR>
</P>
</BODY>
</HTML>